{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class for getting & parsing RSS feeds\n",
    "import feedparser as fp\n",
    "import pandas as pd\n",
    "import html\n",
    "from nltk.tokenize import TreebankWordTokenizer,WhitespaceTokenizer\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import time\n",
    "from guess_language import guess_language\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "WORD = re.compile(r'\\w+')\n",
    "\n",
    "\n",
    "#These functions clean the data in various ways\n",
    "class Cleaner():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.wp = WhitespaceTokenizer()\n",
    "    \n",
    "    @classmethod\n",
    "    def remove_html_tags(cls,text):\n",
    "        import re\n",
    "        clean = re.compile('<.*?>')\n",
    "        return re.sub(clean, '', text)   \n",
    "    @classmethod\n",
    "    def replace_dash(cls,text):\n",
    "        return text.replace('-','')\n",
    "\n",
    "    #This removes non-alphabetical characters and makes everything lower case\n",
    "    @classmethod\n",
    "    def clean(cls,text):\n",
    "        return ''.join(c for c in cls.remove_html_tags(cls.replace_dash(html.unescape(text.lower()))) \n",
    "                       if c in string.ascii_lowercase+' ')\n",
    "    #this tokenizes intelligently\n",
    "\n",
    "    def tokenize(self,text):\n",
    "        #return TreebankWordTokenizer().tokenize(text)\n",
    "        return self.wp.tokenize(text)\n",
    "    #     words = WORD.findall(text)\n",
    "    #     return words\n",
    "    #this removes stopword tokens from a list of tokens\n",
    "    @classmethod\n",
    "    def remove_stop_words(cls,tokens):\n",
    "        return [word for word in tokens if word not in stopwords.words('english')]\n",
    "    #this will clean & tokenize a list of documents.\n",
    "\n",
    "    def preprocess_documents(self,summaries):\n",
    "        return [self.remove_stop_words(self.tokenize(self.clean(s))) for s in summaries]\n",
    "            \n",
    "class NoMoreFeedError(Exception):\n",
    "    pass\n",
    "\n",
    "class FeedCrawler():\n",
    "    def __init__(self,podcast_df,foutloc,identifier=str(int(time.time()))):\n",
    "        self.foutloc = foutloc\n",
    "        self.podcast_df = podcast_df\n",
    "        self.counter = 0\n",
    "        self.feeds = []\n",
    "        self.feedctr = []\n",
    "        self.state = 'parsed'\n",
    "        self.cleaner = Cleaner()\n",
    "        self.filecounter = 0\n",
    "        self.identifier = identifier\n",
    "        \n",
    "    def getNcasts(self):\n",
    "        return self.podcast_df.shape[0]\n",
    "    \n",
    "    #gets the next n feeds in \n",
    "    def getFeed(self,n):\n",
    "        for i in range(0,n):\n",
    "            if(self.counter > self.podcast_df.shape[0]):\n",
    "                raise NoMoreFeedError\n",
    "            url = podcast_df.iloc[self.counter]['feedUrl']\n",
    "            self.feeds.append(self._feed_request(url))\n",
    "            self.counter += 1\n",
    "    \n",
    "    def parseFeeds(self):\n",
    "        parsed_feeds = []\n",
    "        for f in self.feeds:\n",
    "            try:\n",
    "                parsed_feeds.append((f[0],[f[1]['entries'][k]['content'][0]['value'] \n",
    "                                           for k in range(0,len(f[1]['entries']))]))\n",
    "            except KeyError as e:\n",
    "                pass\n",
    "        self.feeds = parsed_feeds\n",
    "        self.state = 'parsed'\n",
    "        print('Parsed!')\n",
    "        \n",
    "    def cleanFeeds(self):\n",
    "        #remove non-english feeds\n",
    "        english_feeds = []\n",
    "        for f in self.feeds:\n",
    "            if(len(f[1])>0):\n",
    "                lang = guess_language(self.cleaner.remove_html_tags(f[1][0]))\n",
    "                if(lang=='en'):\n",
    "                    english_feeds.append(f)\n",
    "        self.feeds = english_feeds\n",
    "        \n",
    "        #clean feeds\n",
    "#         cleaned_feeds = [(f1[0],self.cleaner.preprocess_documents(f1[1])) for f1 in self.feeds]\n",
    "#         self.feeds = cleaned_feeds\n",
    "#         self.state = 'clean'\n",
    "        print('Clean!')\n",
    "    \n",
    "    def _saveFeeds(self):\n",
    "        print('Saving %d feeds...' % len(self.feeds))\n",
    "        with open(foutloc+'feeds_'+str(self.filecounter)+'_'+str(self.identifier)+'.pkl','wb') as fid:\n",
    "            pickle.dump(self.feeds,fid)\n",
    "            self.filecounter += 1\n",
    "    \n",
    "    def _saveProgress(self):\n",
    "        with open(foutloc+'progress.pkl','wb') as fid:\n",
    "            pickle.dump([self.identifier,self.filecounter,self.counter],fid)\n",
    "    \n",
    "    def save(self):\n",
    "        self._saveFeeds()\n",
    "        self._saveProgress()\n",
    "        self._resetFeeds()\n",
    "        \n",
    "    def loadcounters(self):\n",
    "        with open(foutloc+'progress.pkl','rb') as fid:\n",
    "            progress = pickle.load(fid)\n",
    "            self.identifier = progress[0]\n",
    "            self.filecounter = progress[1]\n",
    "            self.counter = progress[2]\n",
    "            \n",
    "    \n",
    "    def _resetFeeds(self):\n",
    "        self.feeds = []\n",
    "        self.state = 'raw'\n",
    "        \n",
    "    def resetCounter(self):\n",
    "        self.counter = 0\n",
    "        \n",
    "    @classmethod \n",
    "    def _feed_request(cls,url):\n",
    "        try:\n",
    "            return (url,fp.parse(url))\n",
    "        except:\n",
    "            print('Error on ' + url)\n",
    "            return (url,None)\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#associate summaries w/ row in podcast_df\n",
    "#load in podcast_df\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#load in itunes_request_db\n",
    "floc = '/home/bmassi/Dropbox/professional/Insight/data/'\n",
    "with open(floc+'raw_itunes_requests1528609065.pkl','rb') as fid:\n",
    "    raw_itunes_requests = pickle.load(fid)\n",
    "#turn everything into a pandas dataframe\n",
    "formatted_results = []\n",
    "bads = []\n",
    "cnames = ['']\n",
    "for rir in raw_itunes_requests:\n",
    "    for p in rir.json()['results']:\n",
    "        if(p['kind']=='podcast'):\n",
    "            formatted_results.append(p)\n",
    "\n",
    "podcast_df = pd.DataFrame(formatted_results)\n",
    "podcast_df = podcast_df.loc[podcast_df['feedUrl'].isnull()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137785\n"
     ]
    }
   ],
   "source": [
    "#filter out podcasts that weren't updated recently (look at fraction of remaining dataset as a function of cutoff)\n",
    "from datetime import datetime,timedelta\n",
    "\n",
    "#parameters for analysis\n",
    "target_cutoff = 45#filter out podcasts that weren't updated for this many days\n",
    "comparator_day = datetime(2018,6,11,14,5,23,424906)#This was \"today\" on Monday 6-11-18\n",
    "\n",
    "podcast_df['releaseDate'] = pd.to_datetime(podcast_df['releaseDate'])\n",
    "podcast_df = podcast_df.loc[podcast_df['releaseDate'] > (comparator_day - timedelta(days=target_cutoff))]\n",
    "\n",
    "print(len(podcast_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized! counter = 500,filecounter = 1,id = 1529184593.957413\n",
      "Parsed!\n",
      "Clean!\n",
      "Saving 340 feeds...\n",
      "Counter=1000/137785 (duration=345.92, step=500)\n",
      "Parsed!\n",
      "Clean!\n",
      "Saving 365 feeds...\n",
      "Counter=1500/137785 (duration=353.64, step=500)\n",
      "Parsed!\n",
      "Clean!\n",
      "Saving 322 feeds...\n",
      "Counter=2000/137785 (duration=392.31, step=500)\n",
      "Parsed!\n",
      "Clean!\n",
      "Saving 348 feeds...\n",
      "Counter=2500/137785 (duration=369.92, step=500)\n",
      "Parsed!\n",
      "Clean!\n",
      "Saving 306 feeds...\n",
      "Counter=3000/137785 (duration=399.55, step=500)\n",
      "Parsed!\n",
      "Clean!\n",
      "Saving 291 feeds...\n",
      "Counter=3500/137785 (duration=348.34, step=500)\n",
      "Parsed!\n",
      "Clean!\n",
      "Saving 303 feeds...\n",
      "Counter=4000/137785 (duration=383.37, step=500)\n",
      "Parsed!\n",
      "Clean!\n",
      "Saving 328 feeds...\n",
      "Counter=4500/137785 (duration=403.08, step=500)\n",
      "Parsed!\n",
      "Clean!\n",
      "Saving 328 feeds...\n",
      "Counter=5000/137785 (duration=387.65, step=500)\n",
      "Parsed!\n",
      "Clean!\n",
      "Saving 349 feeds...\n",
      "Counter=5500/137785 (duration=326.87, step=500)\n",
      "Parsed!\n",
      "Clean!\n",
      "Saving 259 feeds...\n",
      "Counter=6000/137785 (duration=416.46, step=500)\n",
      "Parsed!\n",
      "Clean!\n",
      "Saving 279 feeds...\n",
      "Counter=6500/137785 (duration=356.90, step=500)\n",
      "Parsed!\n",
      "Clean!\n",
      "Saving 310 feeds...\n",
      "Counter=7000/137785 (duration=342.38, step=500)\n",
      "Parsed!\n",
      "Clean!\n",
      "Saving 320 feeds...\n",
      "Counter=7500/137785 (duration=349.24, step=500)\n",
      "Parsed!\n",
      "Clean!\n",
      "Saving 290 feeds...\n",
      "Counter=8000/137785 (duration=346.32, step=500)\n",
      "Parsed!\n",
      "Clean!\n",
      "Saving 302 feeds...\n",
      "Counter=8500/137785 (duration=407.25, step=500)\n",
      "Parsed!\n",
      "Clean!\n",
      "Saving 298 feeds...\n",
      "Counter=9000/137785 (duration=393.65, step=500)\n",
      "Parsed!\n",
      "Clean!\n",
      "Saving 298 feeds...\n",
      "Counter=9500/137785 (duration=358.38, step=500)\n",
      "Parsed!\n",
      "Clean!\n",
      "Saving 349 feeds...\n",
      "Counter=10000/137785 (duration=345.12, step=500)\n",
      "Parsed!\n",
      "Clean!\n",
      "Saving 370 feeds...\n",
      "Counter=10500/137785 (duration=413.75, step=500)\n",
      "Parsed!\n",
      "Clean!\n",
      "Saving 344 feeds...\n",
      "Counter=11000/137785 (duration=328.46, step=500)\n",
      "Parsed!\n",
      "Clean!\n",
      "Saving 312 feeds...\n",
      "Counter=11500/137785 (duration=415.11, step=500)\n",
      "Parsed!\n",
      "Clean!\n",
      "Saving 361 feeds...\n",
      "Counter=12000/137785 (duration=354.62, step=500)\n",
      "Parsed!\n",
      "Clean!\n",
      "Saving 316 feeds...\n",
      "Counter=12500/137785 (duration=621.49, step=500)\n",
      "Parsed!\n",
      "Clean!\n",
      "Saving 307 feeds...\n",
      "Counter=13000/137785 (duration=387.78, step=500)\n"
     ]
    }
   ],
   "source": [
    "#Crawl & scrape feeds\n",
    "import numpy as np\n",
    "import time\n",
    "import socket\n",
    "import feedparser as fp\n",
    "import requests\n",
    "\n",
    "MAX_REQUEST_DURATION = 10 #seconds\n",
    "socket.setdefaulttimeout(MAX_REQUEST_DURATION)\n",
    "\n",
    "step_size = 500#number of feeds to get/save at once\n",
    "foutloc = '/home/bmassi/Dropbox/professional/Insight/data/preprocessed_summaries2/'\n",
    "crawler = FeedCrawler(podcast_df,foutloc)\n",
    "\n",
    "try:\n",
    "    crawler.loadcounters()\n",
    "    print('initialized! counter = %d,filecounter = %d,id = %s' %\n",
    "          (crawler.counter,crawler.filecounter,crawler.identifier))\n",
    "except:\n",
    "    print(\"could not initialize\")\n",
    "    init = 0\n",
    "\n",
    "flag = 1\n",
    "while(flag):\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        crawler.getFeed(step_size)\n",
    "        crawler.parseFeeds()\n",
    "        crawler.cleanFeeds()\n",
    "        crawler.save()\n",
    "    except NoMoreFeedError as e:\n",
    "        flag = 0\n",
    "        print(\"Job's done!\")\n",
    "    stop_time = time.time()\n",
    "    duration = stop_time - start_time\n",
    "    print('Counter=%d/%d (duration=%.2f, step=%d)' % (crawler.counter,crawler.getNcasts(),duration,step_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Clean feeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "floc = '/home/bmassi/Dropbox/professional/Insight/data/preprocessed_summaries2/'\n",
    "with open(floc+'feeds_0_1529184593.957413.pkl','rb') as fid:\n",
    "    prog = pickle.load(fid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:insight]",
   "language": "python",
   "name": "conda-env-insight-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
