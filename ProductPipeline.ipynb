{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These functions clean the data in various ways\n",
    "import re\n",
    "import html\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "import string\n",
    "\n",
    "class Cleaner():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.wp = WhitespaceTokenizer()\n",
    "        self.reclean = re.compile('<.*?>')\n",
    "        \n",
    "    def remove_html_tags(self,text):\n",
    "        return re.sub(self.reclean, ' ', text) \n",
    "    \n",
    "    @classmethod\n",
    "    def replace_newline(cls,text):\n",
    "        return text.replace('\\n',' ')\n",
    "    \n",
    "    @classmethod\n",
    "    def replace_dash(cls,text,on=True):\n",
    "        if(on):\n",
    "            return text.replace('-',' ')\n",
    "        else:\n",
    "            return text\n",
    "\n",
    "    #This removes non-alphabetical characters and makes everything lower case\n",
    "    def clean(self,text,rep_dash=True):\n",
    "        return ''.join(c for c in self.remove_html_tags(self.replace_dash(self.replace_newline(html.unescape(text.lower())),rep_dash)) \n",
    "                       if c in string.ascii_lowercase+' ')\n",
    "    #this tokenizes intelligently\n",
    "\n",
    "    def tokenize(self,text):\n",
    "        #return TreebankWordTokenizer().tokenize(text)\n",
    "        return self.wp.tokenize(text)\n",
    "    #     words = WORD.findall(text)\n",
    "    #     return words\n",
    "    #this removes stopword tokens from a list of tokens\n",
    "    def remove_stop_words(self,tokens):\n",
    "        return [word for word in tokens if word not in stopwords.words('english')]\n",
    "    \n",
    "    #this will clean & tokenize a list of documents.\n",
    "    \n",
    "    def preprocess_input(self,words,rep_dash=True):\n",
    "        return self.remove_stop_words(self.tokenize(self.clean(words,rep_dash)))\n",
    "\n",
    "    def preprocess_documents(self,summaries,rep_dash=True):\n",
    "        return [self.preprocess_input(s,rep_dash) for s in summaries]\n",
    "\n",
    "    def prepare(self,text):\n",
    "        return self.remove_html_tags(html.unescape(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class for managing interactions between model and podcast database.\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn import decomposition,mixture\n",
    "import sklearn\n",
    "from matplotlib import pyplot as plt, rcParams\n",
    "rcParams.update({'font.size': 15})\n",
    "from nltk.corpus import stopwords\n",
    "import feedparser as fp\n",
    "#import sqlite3\n",
    "#from sqlalchemy.orm import scoped_session\n",
    "#from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "class PodcastDB:\n",
    "    #static class variables\n",
    "\n",
    "    #initialize object\n",
    "    def __init__(self,fid,model=None):\n",
    "        if(fid is not None):\n",
    "            self.podcastdb = pickle.load(fid)\n",
    "            #sqlout = self.__querydb('SELECT collectionId,w2v FROM podcasts')\n",
    "            self.w2vs = [v for v in self.podcastdb['w2v'].get_values()]\n",
    "            #self.ids = [id for id in self.podcastdb['collectionId'].get_values()]\n",
    "            self.npodcast = len(self.w2vs)\n",
    "        else:\n",
    "            raise ValueError('Object constructor must be called with a valid file ID')\n",
    "            self.podcastdb = None\n",
    "            self.w2vs = None\n",
    "            #self.ids = None\n",
    "            self.npodcast = 0\n",
    "            \n",
    "        if(isinstance(model,gensim.models.keyedvectors.Word2VecKeyedVectors)):\n",
    "            self.model = model\n",
    "        else:\n",
    "            raise ValueError('Object constructor must be called with a valid model')\n",
    "            self.model = None\n",
    "            \n",
    "        self.comparator = scipy.spatial.distance.cosine\n",
    "        #self.reclean = re.compile('<.*?>')\n",
    "        self.cleaner = Cleaner()\n",
    "\n",
    "            \n",
    "    #primary method. finds podcasts most similar to some word.\n",
    "    def search(self,word,n_outputs=5):\n",
    "\n",
    "        word = self.cleaner.preprocess_input(word)\n",
    "        \n",
    "        #ensures that object is properly initialized\n",
    "        if((self.podcastdb is None) or (self.model is None)):\n",
    "            raise ClassError('Object not properly initialized.')\n",
    "            \n",
    "        if(not word):\n",
    "            raise ValueError('Input contains no valid words.')\n",
    "        \n",
    "        #ADD SQL QUERY HERE\n",
    "        return self.podcastdb.iloc[self.__compare(self._evaluate(word)).argsort()[:n_outputs]]\n",
    "        #return [self.podcastdb.loc[self.podcastdb['collectionId']==thisid] for thisid in bestID]\n",
    "    \n",
    "\n",
    "    #primary method. finds podcasts most similar to some word.\n",
    "    def search_episodes(self,word,n_outputs=3,n_episodes=5,n_most_recent=10):\n",
    "                \n",
    "        #find the best podcasts, evaluate input\n",
    "        pc_match = self.search(word,n_outputs)\n",
    "        u = self._evaluate(self.cleaner.preprocess_input(word,rep_dash=True))\n",
    "        \n",
    "        #get the episodes associated with the best podcasts\n",
    "        #get eps of each matching podcast\n",
    "        ep_data = [self._get_eps(pc_match.iloc[i]['feedUrl']) for i in range(0,len(pc_match))] \n",
    "        #vectorize each episode\n",
    "        ep_vec = [[self._evaluate(self.cleaner.preprocess_input(eps['entries'][i]['content'][0]['value'])) \n",
    "                   for i in range(0,min([n_most_recent,len(eps['entries'])]))] for eps in ep_data]\n",
    "\n",
    "        #get relevant ep data\n",
    "        sorted_eps = [np.array([self.comparator(u,v) for v in ev]).argsort()[:n_outputs] for ev in ep_vec]\n",
    "        \n",
    "        #return the data for the best eps\n",
    "        return pc_match, [[ep_data[i]['entries'][j] for j in sorted_eps[i]]\n",
    "                          for i in range(0,len(ep_data))]\n",
    "\n",
    "    #get the most recent n episodes associated with the best matching podcasts\n",
    "    def _get_eps(self,url):\n",
    "        try:\n",
    "            return fp.parse(url)\n",
    "        except:\n",
    "            print('Error on ' + url)\n",
    "            return (url,None)\n",
    "    \n",
    "    #apply internal model to a single word. \n",
    "    def _evaluate(self,word):\n",
    "        if(isinstance(word,list)):\n",
    "            return self.__evaluate_set(word)\n",
    "        elif(isinstance(word,str)):\n",
    "            #attempt to get vectorial representation of word.\n",
    "            try:\n",
    "                return self.model[word]\n",
    "            except KeyError as e:\n",
    "                return np.full([300,],np.nan)\n",
    "        else:\n",
    "            raise TypeError()\n",
    "            \n",
    "    #apply the model to a set of words and average them. \n",
    "    #this is simply ep2vec from other scripts.\n",
    "    def __evaluate_set(self,words):\n",
    "        #evaluate each word in \n",
    "        n = 0\n",
    "        a = []\n",
    "        for w in words:\n",
    "            #attempt to evaluate vectorial representation of word.\n",
    "            try:\n",
    "                v = self.model[w]\n",
    "                if((np.isnan(v).any() + np.isinf(v).any()) == 0):\n",
    "                    a.append(v)\n",
    "                    n += 1\n",
    "            except KeyError as e:\n",
    "                pass\n",
    "        #if nothing was valid, return nan\n",
    "        if(n==0):\n",
    "            return np.full([300,], np.nan)\n",
    "        #return average\n",
    "        return np.mean(np.array(a),axis=0)\n",
    "    \n",
    "        #compares vector \n",
    "    def __compare(self,u):\n",
    "        \n",
    "        #return distances between vector and all our podcasts.\n",
    "        return np.array([self.comparator(u,v) for v in self.w2vs])\n",
    "    \n",
    "#    def __querydb(self,query):\n",
    "#        this_session = self.podcastdbcur()\n",
    "#        results = this_session.query().from_statement(query).all()\n",
    "#        print('This is the one: %s' % str(results))\n",
    "#        return results\n",
    "#        #return self.podcastdbcur.fetchall()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in podcast df, gensim model, and put it in the database object.\n",
    "floc = '/home/bmassi/Dropbox/professional/Insight/data/'\n",
    "modelfname = 'GoogleNews-vectors-negative300.bin'\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(floc+modelfname, binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate podcast search object\n",
    "dbname = 'podcast_df_subset_BIGDATA_REDUCED.pkl'\n",
    "with open(floc+dbname,'rb') as fid:\n",
    "    podcastdb = PodcastDB(fid=fid,model=word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for displying results\n",
    "from newspaper import Article\n",
    "\n",
    "\n",
    "cleaner = Cleaner()\n",
    "\n",
    "#Returns text or title of an article from article URL. \n",
    "def generateArticleInput(url,title_only=True):\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    if(title_only):\n",
    "        return article.title\n",
    "    else:\n",
    "        return article.text\n",
    "\n",
    "def formatOutput(output):\n",
    "    outstr = ''\n",
    "    for i in range(0,len(output[1])):\n",
    "        outstr += '\\n================================'+output[0].iloc[i]['collectionName']+'================================\\n'\n",
    "        for j in range(0,len(output[1][i])):\n",
    "            outstr += '================' + output[1][i][j].title + '\\n' + cleaner.remove_html_tags(output[1][i][j].summary_detail.value) + '\\n'\n",
    "            \n",
    "    return outstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.549020290374756\n"
     ]
    }
   ],
   "source": [
    "#recommendation pipeline using only title\n",
    "url = 'http://www.breitbart.com/big-government/2018/06/19/donald-trump-democrats-want-more-illegal-immigrants-as-potential-voters/'\n",
    "\n",
    "start_time = time.time()\n",
    "atext = generateArticleInput(url,title_only=True)\n",
    "output = podcastdb.search_episodes(atext)\n",
    "#output = podcastdb.search(atext)\n",
    "stop_time = time.time()\n",
    "duration = stop_time - start_time\n",
    "print(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2971430"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcastdb.podcastdb.shape[0]*55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.500269412994385\n"
     ]
    }
   ],
   "source": [
    "#recommendation pipeline using entire body\n",
    "url = 'http://www.breitbart.com/big-government/2018/06/19/donald-trump-democrats-want-more-illegal-immigrants-as-potential-voters/'\n",
    "\n",
    "start_time = time.time()\n",
    "atext = generateArticleInput(url,False)\n",
    "output = podcastdb.search_episodes(atext)\n",
    "#output = podcastdb.search(atext)\n",
    "stop_time = time.time()\n",
    "duration = stop_time - start_time\n",
    "print(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================Rush Limbaugh Morning Update================================\n",
      "================Rush Limbaugh June 19th 2018\n",
      " Washington Post is in big trouble as more than four hundred employees demand boss Jeff Bezos for higher pay and better benefits.  \n",
      "================Rush Limbaugh June 18th 2018\n",
      " Right-Wing blogger Jennifer Ruben claimed that the Inspector General report proved that Hillary Clinton has a good reason to complain. \n",
      "================Rush Limbaugh June 15th 2018\n",
      " A group of potential Democratic presidential candidates spoke at the We The People summit. Bernie Sanders spoke about universal healthcare and Luis Gutierrez spoke about illegal immigration. \n",
      "\n",
      "================================US of Ed================================\n",
      "================Enemies into friends, friends into enemies\n",
      "\"Something can start from there, where there was nothing before.\" \n",
      "\n",
      "Donald Trump has brought his special brand of negotiation to the summit with Kim Jong-un and come away with a win despite a concession on US / South Korea war games. What worked, and what would Barack Obama or other predecessors have done differently? His diplomacy was less stellar at the G7 summit in Canada and immediately after, mindlessly attacking allies and pulling for Russia. Also talking Ohio's voter purge, the GOP's split on immigration, and a case study for moderate Republicans in Mark Sanford.\n",
      "================Blue Tsunami, a force of nature or a washout?\n",
      "\"It looks like the good guys won this one, and will have a good chance in the regular election.\" This week's primaries were the biggest bunch of elections we'll see until November, so what should we take away from the results? Former Trump campaign manager Paul Manafort is headed to prison on witness tampering charges relating back to the Mueller investigation. What might he go to save his own skin. Also talking North Korea, tough tariffs on American allies, and Donald Trump's bizarre and petty snub to Superbowl winners the Philadelphia Eagles.\n",
      "================Off and running in the 2018 congressional races\n",
      "\"This is an example of the two faces of America right now.\"\n",
      "\n",
      "A punch-drunk GOP, weary from internal division and Trump scandals, faces off against a chipper but at times just as divided, and plausibly overconfident Democratic party as the 2018 congressional races begin. Ed talks potential pitfalls for both parties as well as 1,475 missing migrant kids and Trump's refusals to take responsibility. Also, North Korea, Puerto Rico, and the latest bizarre developments between the White House and the Russia investigation.\n",
      "\n",
      "================================EhRadio================================\n",
      "================Morning moment Tommy Robinson supporter cries and wails June 19 2018\n",
      "Just one report is it fair and balanced I will leave you to decide. Britains fighting for FREEDOM did you ever think you would see this?\n",
      "================Morning moment FREE SPEECH GONE June 18 2018\n",
      "Free speech GONE in Canada and Most Canadians have NO IDEA it's happened\n",
      "================Morning Moment Freedom of speech June 15 2018\n",
      "JUSTIN sneaks into the budget of Canada an illegal measure to ban FREE SPEECH in Canada\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(formatOutput(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['artistId', 'artistName', 'artistViewUrl', 'artworkUrl100',\n",
       "       'artworkUrl30', 'artworkUrl60', 'artworkUrl600',\n",
       "       'collectionCensoredName', 'collectionExplicitness', 'collectionHdPrice',\n",
       "       'collectionId', 'collectionName', 'collectionPrice',\n",
       "       'collectionViewUrl', 'contentAdvisoryRating', 'country', 'currency',\n",
       "       'feedUrl', 'genreIds', 'genres', 'kind', 'primaryGenreName',\n",
       "       'releaseDate', 'trackCensoredName', 'trackCount', 'trackExplicitness',\n",
       "       'trackHdPrice', 'trackHdRentalPrice', 'trackId', 'trackName',\n",
       "       'trackPrice', 'trackRentalPrice', 'trackViewUrl', 'wrapperType', 'w2v'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcastdb.podcastdb.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "floc = '/home/bmassi/Dropbox/professional/Insight/data/'\n",
    "\n",
    "podcastfname = 'podcast_df_subset_BIGDATA_1529347011.pkl'\n",
    "with open(floc+podcastfname,'rb') as fid:\n",
    "    podcastdb = pickle.load(fid)\n",
    "\n",
    "podcastdb = podcastdb.loc[podcastdb['trackCount']>10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['artistId', 'artistName', 'artistViewUrl', 'artworkUrl100',\n",
       "       'artworkUrl30', 'artworkUrl60', 'artworkUrl600',\n",
       "       'collectionCensoredName', 'collectionExplicitness', 'collectionHdPrice',\n",
       "       'collectionId', 'collectionName', 'collectionPrice',\n",
       "       'collectionViewUrl', 'contentAdvisoryRating', 'country', 'currency',\n",
       "       'feedUrl', 'genreIds', 'genres', 'kind', 'primaryGenreName',\n",
       "       'releaseDate', 'trackCensoredName', 'trackCount', 'trackExplicitness',\n",
       "       'trackHdPrice', 'trackHdRentalPrice', 'trackId', 'trackName',\n",
       "       'trackPrice', 'trackRentalPrice', 'trackViewUrl', 'wrapperType', 'w2v'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcastdb.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcastdb = podcastdb[['artistName','collectionExplicitness','collectionId','collectionName',\n",
    "                     'collectionViewUrl','feedUrl','genres','primaryGenreName','releaseDate','trackCount','w2v','artworkUrl30','artworkUrl60','artworkUrl100','artworkUrl600']]\n",
    "podcastfname = 'podcast_df_subset_BIGDATA_REDUCED.pkl'\n",
    "with open(floc+podcastfname,'wb') as fid:\n",
    "    pickle.dump(podcastdb,fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n"
     ]
    }
   ],
   "source": [
    "#Make a SQL database\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "#Connect to database\n",
    "floc = '/home/bmassi/Dropbox/professional/Insight/data/'\n",
    "conn = sqlite3.connect(database=floc+'podcast_database.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "print('loaded')\n",
    "podcastfname = 'podcast_df_subset_BIGDATA_REDUCED.pkl'\n",
    "\n",
    "\n",
    "with open(floc+podcastfname,'rb') as fid:\n",
    "    podcastdf = pickle.load(fid)\n",
    "    podcastdf = podcastdf[['artistName', 'collectionExplicitness', 'collectionId',\n",
    "       'collectionName', 'collectionViewUrl', 'feedUrl',\n",
    "       'primaryGenreName', 'releaseDate', 'trackCount', 'w2v']]\n",
    "    podcastdf['w2v'] = [','.join(str(e) for e in x) for x in podcastdf['w2v'].get_values()]\n",
    "    podcastdf.to_sql('podcasts', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT collectionId,w2v FROM podcasts\n",
    "'''\n",
    "\n",
    "cur.execute(query)\n",
    "sqlout = cur.fetchall()\n",
    "w2v = [np.array([float(e) for e in x[1].split(',')]) for x in sqlout]\n",
    "ids = [x[0] for x in sqlout]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-9478b8d269fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not list"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54026, 10)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go', 'around', 'horn', 'take', 'tour', 'wide', 'world', 'sports', 'takes']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaner = Cleaner()\n",
    "cleaner.preprocess_input('We go around the horn and take a tour through the wide world of sports takes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:insight]",
   "language": "python",
   "name": "conda-env-insight-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
