{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These functions clean the data in various ways\n",
    "import re\n",
    "import html\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class Cleaner():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.wp = WhitespaceTokenizer()\n",
    "        self.reclean = re.compile('<.*?>')\n",
    "        \n",
    "    def remove_html_tags(self,text):\n",
    "        return re.sub(self.reclean, ' ', text) \n",
    "    \n",
    "    @classmethod\n",
    "    def replace_newline(cls,text):\n",
    "        return text.replace('\\n',' ')\n",
    "    \n",
    "    @classmethod\n",
    "    def replace_dash(cls,text,on=True):\n",
    "        if(on):\n",
    "            return text.replace('-',' ')\n",
    "        else:\n",
    "            return text\n",
    "\n",
    "    #This removes non-alphabetical characters and makes everything lower case\n",
    "    def clean(self,text,rep_dash=True):\n",
    "        return ''.join(c for c in self.remove_html_tags(self.replace_dash(self.replace_newline(html.unescape(text.lower())),rep_dash)) \n",
    "                       if c in string.ascii_lowercase+' ')\n",
    "    #this tokenizes intelligently\n",
    "\n",
    "    def tokenize(self,text):\n",
    "        #return TreebankWordTokenizer().tokenize(text)\n",
    "        return self.wp.tokenize(text)\n",
    "    #     words = WORD.findall(text)\n",
    "    #     return words\n",
    "    #this removes stopword tokens from a list of tokens\n",
    "    def remove_stop_words(self,tokens):\n",
    "        return [word for word in tokens if word not in stopwords.words('english')]\n",
    "    \n",
    "    #this will clean & tokenize a list of documents.\n",
    "    \n",
    "    def preprocess_input(self,words,rep_dash=True):\n",
    "        return self.remove_stop_words(self.tokenize(self.clean(words,rep_dash)))\n",
    "\n",
    "    def preprocess_documents(self,summaries,rep_dash=True):\n",
    "        return [self.preprocess_input(s,rep_dash) for s in summaries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class for managing interactions between model and podcast database.\n",
    "import pickle\n",
    "import time\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn import decomposition,mixture\n",
    "import sklearn\n",
    "from matplotlib import pyplot as plt, rcParams\n",
    "import matplotlib\n",
    "rcParams.update({'font.size': 15})\n",
    "from nltk.tokenize import TreebankWordTokenizer,WhitespaceTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import feedparser as fp\n",
    "import sqlite3\n",
    "\n",
    "class PodcastDB:\n",
    "    #static class variables\n",
    "    wp = WhitespaceTokenizer()\n",
    "\n",
    "    #initialize object\n",
    "    def __init__(self,dbfloc=None,model=None):\n",
    "        if(dbfloc is not None):\n",
    "            conn = sqlite3.connect(database=dbfloc)\n",
    "            self.podcastdbcur = conn.cursor()\n",
    "            sqlout = self.__querydb('SELECT collectionId,w2v FROM podcasts')\n",
    "            self.w2vs = [np.array([float(e) for e in x[1].split(',')]) for x in sqlout]\n",
    "            self.ids = [x[0] for x in sqlout]\n",
    "            self.npodcast = len(self.ids)\n",
    "        else:\n",
    "            raise ValueError('Object constructor must be called with a valid file ID')\n",
    "            self.podcastdbcur = None\n",
    "            self.w2vs = None\n",
    "            self.ids = None\n",
    "            self.npodcast = 0\n",
    "            \n",
    "        if(isinstance(model,gensim.models.keyedvectors.Word2VecKeyedVectors)):\n",
    "            self.model = model\n",
    "        else:\n",
    "            raise ValueError('Object constructor must be called with a valid model')\n",
    "            self.model = None\n",
    "            \n",
    "        self.comparator = scipy.spatial.distance.cosine\n",
    "        #self.reclean = re.compile('<.*?>')\n",
    "        self.cleaner = Cleaner()\n",
    "\n",
    "            \n",
    "    #primary method. finds podcasts most similar to some word.\n",
    "    def search(self,word,n_outputs=5):\n",
    "        \n",
    "        word = self.cleaner.preprocess_input(word)\n",
    "        \n",
    "        #ensures that object is properly initialized\n",
    "        if((self.podcastdbcur is None) or (self.model is None)):\n",
    "            raise ClassError('Object not properly initialized.')\n",
    "            \n",
    "        if(not word):\n",
    "            raise ValueError('Input contains no valid words.')\n",
    "        \n",
    "        #ADD SQL QUERY HERE\n",
    "        bestID = [self.ids[id] for id in self.__compare(self._evaluate(word)).argsort()[:n_outputs]]\n",
    "        return pd.DataFrame(self.__querydb('SELECT collectionName,feedUrl,collectionId FROM podcasts WHERE collectionId in ('\n",
    "                         +','.join(str(id) for id in bestID) + ');'),columns=['collectionName','feedUrl','id'])\n",
    "    \n",
    "\n",
    "    #primary method. finds podcasts most similar to some word.\n",
    "    def search_episodes(self,word,n_outputs=3,n_episodes=5,n_most_recent=10):\n",
    "                \n",
    "        #find the best podcasts, evaluate input\n",
    "        pc_match = self.search(word,n_outputs)\n",
    "        u = self._evaluate(self.cleaner.preprocess_input(word,rep_dash=True))\n",
    "        \n",
    "        #get the episodes associated with the best podcasts\n",
    "        #get eps of each matching podcast\n",
    "        ep_data = [self._get_eps(pc_match.iloc[i]['feedUrl']) for i in range(0,len(pc_match))] \n",
    "        #vectorize each episode\n",
    "        ep_vec = [[self._evaluate(self.cleaner.preprocess_input(eps['entries'][i]['content'][0]['value'])) \n",
    "                   for i in range(0,min([n_most_recent,len(eps['entries'])]))] for eps in ep_data]\n",
    "\n",
    "        #get relevant ep data\n",
    "        sorted_eps = [np.array([self.comparator(u,v) for v in ev]).argsort()[:n_outputs] for ev in ep_vec]\n",
    "        \n",
    "        #return the data for the best eps\n",
    "        return pc_match, [[e for e in eps['entries'][0:min([len(eps['entries']),n_outputs])]]\n",
    "                          for eps in ep_data]\n",
    "\n",
    "    #get the most recent n episodes associated with the best matching podcasts\n",
    "    def _get_eps(self,url):\n",
    "        try:\n",
    "            return fp.parse(url)\n",
    "        except:\n",
    "            print('Error on ' + url)\n",
    "            return (url,None)\n",
    "    \n",
    "    #apply internal model to a single word. \n",
    "    def _evaluate(self,word):\n",
    "        if(isinstance(word,list)):\n",
    "            return self.__evaluate_set(word)\n",
    "        elif(isinstance(word,str)):\n",
    "            #attempt to get vectorial representation of word.\n",
    "            try:\n",
    "                return self.model[word]\n",
    "            except KeyError as e:\n",
    "                return np.full([300,],np.nan)\n",
    "        else:\n",
    "            raise TypeError()\n",
    "            \n",
    "    #apply the model to a set of words and average them. \n",
    "    #this is simply ep2vec from other scripts.\n",
    "    def __evaluate_set(self,words):\n",
    "        #evaluate each word in \n",
    "        n = 0\n",
    "        a = []\n",
    "        for w in words:\n",
    "            #attempt to evaluate vectorial representation of word.\n",
    "            try:\n",
    "                v = self.model[w]\n",
    "                if((np.isnan(v).any() + np.isinf(v).any()) == 0):\n",
    "                    a.append(v)\n",
    "                    n += 1\n",
    "            except KeyError as e:\n",
    "                pass\n",
    "        #if nothing was valid, return nan\n",
    "        if(n==0):\n",
    "            return np.full([300,], np.nan)\n",
    "        #return average\n",
    "        return np.mean(np.array(a),axis=0)\n",
    "    \n",
    "        #compares vector \n",
    "    def __compare(self,u):\n",
    "        \n",
    "        #return distances between vector and all our podcasts.\n",
    "        return np.array([self.comparator(u,v) for v in self.w2vs])\n",
    "    \n",
    "    def __querydb(self,query):\n",
    "        self.podcastdbcur.execute(query)\n",
    "        return self.podcastdbcur.fetchall()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in podcast df, gensim model, and put it in the database object.\n",
    "floc = '/home/bmassi/Dropbox/professional/Insight/data/'\n",
    "modelfname = 'GoogleNews-vectors-negative300.bin'\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(floc+modelfname, binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate podcast search object\n",
    "dbname = 'podcast_database.db'\n",
    "podcastdb = PodcastDB(dbfloc=floc+dbname,model=word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for displying results\n",
    "from newspaper import Article\n",
    "\n",
    "\n",
    "cleaner = Cleaner()\n",
    "\n",
    "#Returns text or title of an article from article URL. \n",
    "def generateArticleInput(url,title_only=True):\n",
    "    article = Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    if(title_only):\n",
    "        return article.title\n",
    "    else:\n",
    "        return article.text\n",
    "\n",
    "def formatOutput(output):\n",
    "    outstr = ''\n",
    "    for i in range(0,len(output[1])):\n",
    "        outstr += '\\n================================'+output[0].iloc[i]['collectionName']+'================================\\n'\n",
    "        for j in range(0,len(output[1][i])):\n",
    "            outstr += '================' + output[1][i][j].title + '\\n' + cleaner.remove_html_tags(output[1][i][j].summary_detail.value) + '\\n'\n",
    "            \n",
    "    return outstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.633759021759033\n"
     ]
    }
   ],
   "source": [
    "#recommendation pipeline using only title\n",
    "url = 'http://www.breitbart.com/big-government/2018/06/19/donald-trump-democrats-want-more-illegal-immigrants-as-potential-voters/'\n",
    "\n",
    "start_time = time.time()\n",
    "atext = generateArticleInput(url,title_only=True)\n",
    "output = podcastdb.search_episodes(atext)\n",
    "#output = podcastdb.search(atext)\n",
    "stop_time = time.time()\n",
    "duration = stop_time - start_time\n",
    "print(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================Capitol Hill Brief================================\n",
      "================What the Dem Outrage Machine Won’t Tell You About the Family Border Crisis | Capitol Hill Brief\n",
      " &nbsp;The open-borders misinformation machine is firing on all cylinders over the Trump administration&rsquo;s immigration enforcement policies concerning families who illegally cross our borders. Nate&rsquo;s here to bring you the facts. \n",
      " &nbsp;The open-borders misinformation machine is firing on all cylinders over the Trump administration&rsquo;s immigration enforcement policies concerning families who illegally cross our borders. Nate&rsquo;s here to bring you the facts. \n",
      "================Rep. Jordan: What the Left Gets Wrong About the Bombshell FBI Report | Capitol Hill Brief\n",
      " The pro-deep-state Left and the legacy media want you to believe that the explosive new report detailing rampant anti-Trump bias at the FBI is a big nothingburger. Rep. Jim Jordan is having absolutely NONE of that nonsense. \n",
      " Copyright CRTV. All rights reserved. \n",
      "================Sen. Lankford: How to Disarm North Korea and Help Free Its People | Capitol Hill Brief\n",
      " Kim Jong Un's atrocious human rights abuses often get less attention than American national security concerns about the North Korean regime. Nate Madden speaks with Sen. James Lankford about the importance of fighting for religious freedom in North Korea and how President Trump&rsquo;s new approach can make it happen. \n",
      " Copyright CRTV. All rights reserved. \n",
      "\n",
      "================================Here’s The Deal================================\n",
      "================Trump Derangement Syndrome, FBI Edition!  | Here's The Deal\n",
      " The DOJ has been caught red-handed in explicit political bias and yet absolved itself in the latest report. Unbelievable. The conclusion directly contradicts the report itself. They really do think you are this stupid. \n",
      " Copyright CRTV. All rights reserved. \n",
      "================We Are Making America Great Again! | Here's The Deal\n",
      " God bless this magnificent country. Our economy is booming, our market is thriving, we've brought North Korea to the table, and we are Making America Great Again! \n",
      " Copyright CRTV. All rights reserved. \n",
      "================That “Blue Wave” Just Can’t Happen with Trump’s Economy | Here's The Deal\n",
      " Trump&rsquo;s economy is booming, but the Left continues to spin the truth, downplay the facts, and even wish for a recession. We know better! Sorry, Dems, that &ldquo;Blue Wave&rdquo; isn&rsquo;t going to happen. America is smarter than you think. \n",
      " &nbsp; \n",
      " Copyright CRTV. All rights reserved. \n",
      "\n",
      "================================Rush Limbaugh Morning Update================================\n",
      "================Rush Limbaugh June 19th 2018\n",
      " Washington Post is in big trouble as more than four hundred employees demand boss Jeff Bezos for higher pay and better benefits.  \n",
      "================Rush Limbaugh June 18th 2018\n",
      " Right-Wing blogger Jennifer Ruben claimed that the Inspector General report proved that Hillary Clinton has a good reason to complain. \n",
      "================Rush Limbaugh June 15th 2018\n",
      " A group of potential Democratic presidential candidates spoke at the We The People summit. Bernie Sanders spoke about universal healthcare and Luis Gutierrez spoke about illegal immigration. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(formatOutput(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.500269412994385\n"
     ]
    }
   ],
   "source": [
    "#recommendation pipeline using entire body\n",
    "url = 'http://www.breitbart.com/big-government/2018/06/19/donald-trump-democrats-want-more-illegal-immigrants-as-potential-voters/'\n",
    "\n",
    "start_time = time.time()\n",
    "atext = generateArticleInput(url,False)\n",
    "output = podcastdb.search_episodes(atext)\n",
    "#output = podcastdb.search(atext)\n",
    "stop_time = time.time()\n",
    "duration = stop_time - start_time\n",
    "print(duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================Rush Limbaugh Morning Update================================\n",
      "================Rush Limbaugh June 19th 2018\n",
      " Washington Post is in big trouble as more than four hundred employees demand boss Jeff Bezos for higher pay and better benefits.  \n",
      "================Rush Limbaugh June 18th 2018\n",
      " Right-Wing blogger Jennifer Ruben claimed that the Inspector General report proved that Hillary Clinton has a good reason to complain. \n",
      "================Rush Limbaugh June 15th 2018\n",
      " A group of potential Democratic presidential candidates spoke at the We The People summit. Bernie Sanders spoke about universal healthcare and Luis Gutierrez spoke about illegal immigration. \n",
      "\n",
      "================================US of Ed================================\n",
      "================Enemies into friends, friends into enemies\n",
      "\"Something can start from there, where there was nothing before.\" \n",
      "\n",
      "Donald Trump has brought his special brand of negotiation to the summit with Kim Jong-un and come away with a win despite a concession on US / South Korea war games. What worked, and what would Barack Obama or other predecessors have done differently? His diplomacy was less stellar at the G7 summit in Canada and immediately after, mindlessly attacking allies and pulling for Russia. Also talking Ohio's voter purge, the GOP's split on immigration, and a case study for moderate Republicans in Mark Sanford.\n",
      "================Blue Tsunami, a force of nature or a washout?\n",
      "\"It looks like the good guys won this one, and will have a good chance in the regular election.\" This week's primaries were the biggest bunch of elections we'll see until November, so what should we take away from the results? Former Trump campaign manager Paul Manafort is headed to prison on witness tampering charges relating back to the Mueller investigation. What might he go to save his own skin. Also talking North Korea, tough tariffs on American allies, and Donald Trump's bizarre and petty snub to Superbowl winners the Philadelphia Eagles.\n",
      "================Off and running in the 2018 congressional races\n",
      "\"This is an example of the two faces of America right now.\"\n",
      "\n",
      "A punch-drunk GOP, weary from internal division and Trump scandals, faces off against a chipper but at times just as divided, and plausibly overconfident Democratic party as the 2018 congressional races begin. Ed talks potential pitfalls for both parties as well as 1,475 missing migrant kids and Trump's refusals to take responsibility. Also, North Korea, Puerto Rico, and the latest bizarre developments between the White House and the Russia investigation.\n",
      "\n",
      "================================EhRadio================================\n",
      "================Morning moment Tommy Robinson supporter cries and wails June 19 2018\n",
      "Just one report is it fair and balanced I will leave you to decide. Britains fighting for FREEDOM did you ever think you would see this?\n",
      "================Morning moment FREE SPEECH GONE June 18 2018\n",
      "Free speech GONE in Canada and Most Canadians have NO IDEA it's happened\n",
      "================Morning Moment Freedom of speech June 15 2018\n",
      "JUSTIN sneaks into the budget of Canada an illegal measure to ban FREE SPEECH in Canada\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(formatOutput(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['artistId', 'artistName', 'artistViewUrl', 'artworkUrl100',\n",
       "       'artworkUrl30', 'artworkUrl60', 'artworkUrl600',\n",
       "       'collectionCensoredName', 'collectionExplicitness', 'collectionHdPrice',\n",
       "       'collectionId', 'collectionName', 'collectionPrice',\n",
       "       'collectionViewUrl', 'contentAdvisoryRating', 'country', 'currency',\n",
       "       'feedUrl', 'genreIds', 'genres', 'kind', 'primaryGenreName',\n",
       "       'releaseDate', 'trackCensoredName', 'trackCount', 'trackExplicitness',\n",
       "       'trackHdPrice', 'trackHdRentalPrice', 'trackId', 'trackName',\n",
       "       'trackPrice', 'trackRentalPrice', 'trackViewUrl', 'wrapperType', 'w2v'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcastdb.podcastdb.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "floc = '/home/bmassi/Dropbox/professional/Insight/data/'\n",
    "\n",
    "podcastfname = 'podcast_df_subset_BIGDATA_1529347011.pkl'\n",
    "with open(floc+podcastfname,'rb') as fid:\n",
    "    podcastdb = pickle.load(fid)\n",
    "podcastdb = podcastdb[['artistName','collectionExplicitness','collectionId','collectionName',\n",
    "                     'collectionViewUrl','feedUrl','genres','primaryGenreName','releaseDate','trackCount','w2v']]\n",
    "\n",
    "podcastdb = podcastdb.loc[podcastdb['trackCount']>10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "podcastfname = 'podcast_df_subset_BIGDATA_REDUCED.pkl'\n",
    "with open(floc+podcastfname,'wb') as fid:\n",
    "    pickle.dump(podcastdb,fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n"
     ]
    }
   ],
   "source": [
    "#Make a SQL database\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "#Connect to database\n",
    "floc = '/home/bmassi/Dropbox/professional/Insight/data/'\n",
    "conn = sqlite3.connect(database=floc+'podcast_database.db')\n",
    "cur = conn.cursor()\n",
    "\n",
    "print('loaded')\n",
    "podcastfname = 'podcast_df_subset_BIGDATA_REDUCED.pkl'\n",
    "\n",
    "\n",
    "with open(floc+podcastfname,'rb') as fid:\n",
    "    podcastdf = pickle.load(fid)\n",
    "    podcastdf = podcastdf[['artistName', 'collectionExplicitness', 'collectionId',\n",
    "       'collectionName', 'collectionViewUrl', 'feedUrl',\n",
    "       'primaryGenreName', 'releaseDate', 'trackCount', 'w2v']]\n",
    "    podcastdf['w2v'] = [','.join(str(e) for e in x) for x in podcastdf['w2v'].get_values()]\n",
    "    podcastdf.to_sql('podcasts', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT collectionId,w2v FROM podcasts\n",
    "'''\n",
    "\n",
    "cur.execute(query)\n",
    "sqlout = cur.fetchall()\n",
    "w2v = [np.array([float(e) for e in x[1].split(',')]) for x in sqlout]\n",
    "ids = [x[0] for x in sqlout]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-9478b8d269fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not list"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54026, 10)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:insight]",
   "language": "python",
   "name": "conda-env-insight-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
